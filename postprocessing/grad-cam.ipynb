{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import scipy\n",
    "import subprocess\n",
    "from natsort import natsorted\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import torch\n",
    "sys.path.append(\"../utls\")\n",
    "sys.path.append(\"../utls\")\n",
    "sys.path.append(\"../.\")\n",
    "sys.path.append(\"../models\")\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import Subset\n",
    "import torchvision\n",
    "import torchvision.transforms\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from utls.preprocessing import *\n",
    "from utls.postprocessing import get_dataset, generate_hist_df\n",
    "from InterfaceDeclaration import LPBFInterface\n",
    "from models.MLUtls import fade_in_out, standardize_tensor, getKFoldCrossValidationIndexes, train_log, transform_ft, dataset_by_cross_validation, labels_by_classes, get_current_fold_and_hist, LPBFDataset\n",
    "from models.MLModels import SVMModel, CNN_Base_1D_Model, ResNet15_1D_Model\n",
    "\n",
    "\n",
    "alphabet = list(string.ascii_lowercase)\n",
    "\n",
    "project_name = [\"MuSIC\", \"MaPS\", \"MuSIC_EXP1\"]\n",
    "if os.name == \"posix\":\n",
    "    data_dir = subprocess.getoutput(\"echo $DATADIR\")\n",
    "elif os.name == \"nt\":\n",
    "    data_dir = subprocess.getoutput(\"echo %datadir%\")\n",
    "music_dir = os.path.join(data_dir, \"MuSIC\")\n",
    "if not os.path.exists(music_dir):\n",
    "    project_name[0] = \"2024-MUSIC\"\n",
    "daq_dir = os.path.join(data_dir, *project_name, \"Acoustic Monitoring\")\n",
    "lmq_dir = os.path.join(data_dir, *project_name, \"LMQ Monitoring\")\n",
    "del music_dir\n",
    "\n",
    "sampling_rate_daq: int = int(1.25 * 1e6)\n",
    "sampling_rate_lmq: int = int(0.1 * 1e6)\n",
    "tdms_daq_list = natsorted(\n",
    "    [i for i in os.listdir(daq_dir) if i.split(\".\")[-1] == \"tdms\"]\n",
    ")\n",
    "bin_lmq_list = natsorted([i for i in os.listdir(lmq_dir) if i.split(\".\")[-1] == \"bin\"])\n",
    "lmq_channel_name = [\n",
    "    \"Vector ID\",\n",
    "    \"meltpooldiode\",\n",
    "    \"X Coordinate\",\n",
    "    \"Y Coordinate\",\n",
    "    \"Laser power\",\n",
    "    \"Spare\",\n",
    "    \"Laser diode\",\n",
    "    \"Varioscan(focal length)\",\n",
    "]\n",
    "process_regime = [\n",
    "    [0,    59, \"Base\"  ], \n",
    "    [60,  129, \"KH\"    ], \n",
    "    [130, 199, \"Normal\"], \n",
    "    [200, 269, \"RLoF\"  ], \n",
    "    [269, 339, \"LoF\"   ] \n",
    "]\n",
    "\n",
    "dataset,sc_power,le_direction,le_speed,le_region = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs=50\n",
    "his_dir = os.path.join(os.pardir,\"lfs\",\"weights/hist\")\n",
    "snap_dir = os.path.join(os.pardir,\"lfs\",\"weights/\")\n",
    "model_name= [\"CNN\"]\n",
    "acoustic_type =['ae','mic'] \n",
    "context_type = []\n",
    "# context_type = ['energy']\n",
    "output_type = ['direction']\n",
    "folds = 10\n",
    "\n",
    "df = generate_hist_df(his_dir,model_name,acoustic_type,context_type,output_type,folds,max_epochs)\n",
    "df['Input Type'] = df['Input type'].str.replace('+', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(model_name, input_type, output_type, time_series_length=5888):\n",
    "    meta_data_size = len(input_type.split(\"+\"))-1\n",
    "    # time_series_length = 5888\n",
    "    # print(f\"Input type: {input_type}\")\n",
    "    # print(f\"Output type: {output_type}\")\n",
    "    # print(f\"Lenght of time series data: {time_series_length}\")\n",
    "    # print(f\"Lenght of context data: {meta_data_size}\")\n",
    "    # print(f\"Model name: {model_name}\")\n",
    "\n",
    "    num_classes = 1\n",
    "    if output_type == \"regime\":\n",
    "        num_classes = 4\n",
    "    if output_type == \"defect\":\n",
    "        num_classes = 2\n",
    "    if output_type == \"direction\":\n",
    "        num_classes = 5\n",
    "    if output_type == \"position\":\n",
    "        num_classes = 5\n",
    "\n",
    "    if model_name == \"SVM\":\n",
    "        model = SVMModel(time_series_length+meta_data_size,num_classes=num_classes).double()\n",
    "    if model_name == \"CNN\":\n",
    "        model = CNN_Base_1D_Model(time_series_length=time_series_length, meta_data_size=meta_data_size,num_classes=num_classes).double()\n",
    "    if model_name == \"Res15\":\n",
    "        model = ResNet15_1D_Model(time_series_length=time_series_length, meta_data_size=meta_data_size,num_classes=num_classes).double()\n",
    "    return model\n",
    "\n",
    "def read_trained_model(snap_dir,model_name, acoustic_type, context_type,output_type,folds,max_epochs):\n",
    "    context_combinations = []\n",
    "    for r in range(len(context_type) + 1): \n",
    "        context_combinations.extend(itertools.combinations(context_type, r))\n",
    "    train_acc_list = []\n",
    "    test_acc_list = []\n",
    "    fold_i_list = []\n",
    "    inputs_list = []\n",
    "    outputs_list = []\n",
    "    model_list = []\n",
    "\n",
    "    for _output in output_type:\n",
    "        for _model in model_name:\n",
    "            for acoustic in acoustic_type:\n",
    "                for context in context_combinations:\n",
    "                    if len(context)>0:\n",
    "                        inputs = f\"{acoustic}+{'+'.join(list(context))}\"\n",
    "                    else:\n",
    "                        inputs = acoustic\n",
    "                    \n",
    "                    for fold in range(folds):\n",
    "                        file_path = f\"{_model}_classification_input_{inputs}_output_{_output}_roi_time10_roi_radius3_fold{fold}_of_folds10.pt\"\n",
    "                        file_path = os.path.join(snap_dir,file_path)\n",
    "                        model = make_model(_model, inputs, _output)\n",
    "    return model,file_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 11760]), torch.Size([8, 5880]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_list = []\n",
    "_cube_position, _laser_power, _scanning_speed, _regime_info, _print_direction, _mic, _ae, _defect_labels = next(iter(data_loader))\n",
    "time_series = (transform_ft()(standardize_tensor(_mic))).double()\n",
    "meta_list.append(_laser_power.double())\n",
    "_ae.shape, time_series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,file_path = read_trained_model(snap_dir,model_name,acoustic_type,context_type,output_type,folds,max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = torch.load(file_path, map_location=f\"cuda:0\", weights_only=True)\n",
    "_state_dict = snapshot[\"model_state_dict\"]\n",
    "model.load_state_dict(_state_dict)\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(time_series.to('cuda'),meta_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_type, output_type, time_series_length=5888):\n",
    "    meta_data_size = len(input_type.split(\"+\"))-1\n",
    "    # time_series_length = 5888\n",
    "    print(f\"Input type: {input_type}\")\n",
    "    print(f\"Output type: {output_type}\")\n",
    "    print(f\"Lenght of time series data: {time_series_length}\")\n",
    "    print(f\"Lenght of context data: {meta_data_size}\")\n",
    "\n",
    "    num_classes = 1\n",
    "    if output_type == \"regime\":\n",
    "        num_classes = 4\n",
    "    if output_type == \"defect\":\n",
    "        num_classes = 2\n",
    "    if output_type == \"direction\":\n",
    "        num_classes = 5\n",
    "    if output_type == \"position\":\n",
    "        num_classes = 5\n",
    "\n",
    "    if model_name == \"SVM\":\n",
    "        model = SVMModel(time_series_length+meta_data_size,num_classes=num_classes).double()\n",
    "    if model_name == \"CNN\":\n",
    "        model = CNN_Base_1D_Model(time_series_length=time_series_length, meta_data_size=meta_data_size,num_classes=num_classes).double()\n",
    "    if model_name == \"Res15\":\n",
    "        model = ResNet15_1D_Model(time_series_length=time_series_length, meta_data_size=meta_data_size,num_classes=num_classes).double()\n",
    "    return model\n",
    "# make_model(acoustic, _output)\n",
    "# model.load_state_dict(_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
